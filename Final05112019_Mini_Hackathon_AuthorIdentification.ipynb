{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final05112019-Mini_Hackathon_AuthorIdentification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chamalar/IIIT-H-AIML/blob/New-Branch/Final05112019_Mini_Hackathon_AuthorIdentification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "74lzxaiQ9-9s"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Advanced Certification in AIML\n",
        "## A Program by IIIT-H and TalentSprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4mJKZzgrMLp",
        "colab_type": "text"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiWKLfqkrOB6",
        "colab_type": "text"
      },
      "source": [
        "The problem is to identify the author of a  book from a given list of possible authors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar8wMsXSCUMo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6ElufS5-9-99"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dCHP0foL9--C"
      },
      "source": [
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* Preprocess the text\n",
        "* Write an algorithm to identify author of a given book\n",
        "* Use NLTK package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCz5oobSruGF",
        "colab_type": "text"
      },
      "source": [
        "## Background "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZWNTcd4oo-D",
        "colab_type": "text"
      },
      "source": [
        "Author identification is the task of identifying the author of a given text. It can be considered as a typical classification problem, where a set of books with known authors are used for training. The aim is to automatically determine the corresponding author of an anonymous text. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQEA97zTlTa",
        "colab_type": "text"
      },
      "source": [
        "## Grading = 25 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9fYwJv9T9--K"
      },
      "source": [
        "## Setup Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tOGbD4Wa9--N",
        "colab": {}
      },
      "source": [
        "#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"P181902237\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BQf6sWOR9--S",
        "colab": {}
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9515779425\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "HPdGmFba9--f",
        "outputId": "f460f6ab-6686-43db-a4d2-f120110167ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "\n",
        "from IPython import get_ipython\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook=\"BLR_M1_Mini_Hackathon_AuthorIdentification\" #name of the notebook\n",
        "Answer = \"This notebook is graded by mentors on the day of hackathon\"\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "   ipython.magic(\"sx wget https://www.dropbox.com/s/9xivz2pox1i83td/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin?dl=1\")\n",
        "   ipython.magic(\"sx mv AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin?dl=1 AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin\")\n",
        "   print (\"Setup completed successfully\")\n",
        "   return\n",
        "\n",
        "def submit_notebook():\n",
        "    \n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook}\n",
        "\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      print(\"Your submission is successful.\")\n",
        "      print(\"Ref Id:\", submission_id)\n",
        "      print(\"Date of submission: \", r[\"date\"])\n",
        "      print(\"Time of submission: \", r[\"time\"])\n",
        "      print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n",
        "      print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "      return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if Additional: return Additional      \n",
        "    else: raise NameError('')\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "  \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup completed successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ioUZ8pJa9--n"
      },
      "source": [
        "### NOTE: Please feel free to use ML libraries such as Sklearn, NLTK etc whereever applicable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ipknN3nY9--r"
      },
      "source": [
        "## Stage 1: Sentence Level Classification\n",
        "\n",
        "You can choose any two authors books from the below list \n",
        "\n",
        "* Shakespeare\n",
        "* Austen\n",
        "* Chesterton \n",
        "\n",
        "The books of the authors are mentioned in **Gutenberg Corpus**.\n",
        "\n",
        "Click on the [link](https://www.nltk.org/book/ch02.html) for reference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR6_IRo7vTrs",
        "colab_type": "text"
      },
      "source": [
        "### Downloading the required nltk Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7BOKJN039--v",
        "outputId": "1efb1ffb-b8a0-4486-e687-5dc08747ae67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YCTPinoX9--0"
      },
      "source": [
        "### 1 Mark-> a) Access a particular book by the chosen author. Find number of sentences and words (tokens) present in that book: \n",
        "\n",
        "Instructions on accessing the Corpus can be found at the following [link](https://www.nltk.org/book/ch02.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h7o97fCZ9--3",
        "outputId": "2c762bd6-a44c-4764-e60e-521a83558534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#####YOUR CODE HERE###############\n",
        "import nltk\n",
        "nltk.corpus.gutenberg.fileids()\n",
        "shakespeare = nltk.corpus.gutenberg.words('chesterton-brown.txt')\n",
        "len(shakespeare)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86063"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4bLZIvs3Ae70"
      },
      "source": [
        "#Before moving ahead choose your two authors based on the your team-nbr: <br/>\n",
        "**1.** <br/>\n",
        "Team=1,6,11,16,21,26 &nbsp;&nbsp;&nbsp;         Author-A Vs Author-B <br />\n",
        "Team=2,7,12,17,22,27 &nbsp;&nbsp;&nbsp;         Author-B Vs Author-C <br />\n",
        "Team=3,8,13,18,23,28 &nbsp;&nbsp;&nbsp;         Author-C Vs Author-D <br />\n",
        "Team=4,9,14,19,24 &nbsp;&nbsp;&nbsp;         Author-D Vs Author-E <br />\n",
        "Team=5,10,15,20,25    &nbsp;&nbsp;&nbsp;         Author-E Vs Author-F <br />\n",
        "\n",
        "**2.** Link for each of the author to be used for your problem: <br />\n",
        "\n",
        "Author-A &nbsp;&nbsp; G.K.Chesterton <br />\n",
        "\n",
        "*   Author-A -> Rudyard Kipling ->  [Short Stories Collection](http://www.gutenberg.org/files/2781/2781-0.txt) &nbsp;&nbsp;\n",
        "*   Author-B -> Anton Chekhov [Short Stories Collection](http://www.gutenberg.org/files/1732/1732-0.txt) &nbsp;&nbsp;\n",
        "*   Author-C -> Guy De Maupassant [Short Stories Collection](http://www.gutenberg.org/cache/epub/21327/pg21327.txt)&nbsp;&nbsp;\n",
        "*   Author-D -> O Henry [Short Stories Collection](http://www.gutenberg.org/cache/epub/2141/pg2141.txt) &nbsp;&nbsp;\n",
        "*   Author-E -> Mark Twain [Short Stories Collection](http://www.gutenberg.org/files/245/245-0.txt)&nbsp;&nbsp;\n",
        "*   Author-F -> Saki [Short Stories Collection](http://www.gutenberg.org/files/1477/1477-0.txt)&nbsp;&nbsp;\n",
        "\n",
        "**Downloading raw text from Gutenberg :**\n",
        " Hint: Refer section \"Electronic Books\" in the following  [link](https://www.nltk.org/book/ch03.html) for the instructions.  \n",
        " \n",
        " \n",
        " **Ensure you appropriately split the above text into multiple short stories, which will be your training data - 2 Marks  (Hint: You may use raw.find() and raw.rfind() in the  [link](https://www.nltk.org/book/ch03.html)   to find appropriate index of the start and end location)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A2gBlFPHhpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib import request\n",
        "\n",
        "Rudyard = \"http://www.gutenberg.org/files/2781/2781-0.txt\"\n",
        "\n",
        "Anton = \"http://www.gutenberg.org/files/1732/1732-0.txt\"\n",
        "\n",
        "response_Rudyard = request.urlopen(Rudyard)\n",
        "response_Anton = request.urlopen(Anton)\n",
        "\n",
        "raw_Rudyard = response_Rudyard.read().decode('utf8')\n",
        "raw_Anton = response_Anton.read().decode('utf8')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_bJtb4zRWM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=raw_Rudyard.find(\"HOW THE WHALE GOT HIS THROAT\")\n",
        "e=raw_Rudyard.find(\"THE BUTTERFLY THAT STAMPED\")\n",
        "#l,e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8btCDjCReia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp=len(\"THE BUTTERFLY THAT STAMPED\")\n",
        "TOC_Rudyard=raw_Rudyard[l:(e+temp)]\n",
        "#print(TOC_Rudyard)\n",
        "#print(type(TOC_Rudyard))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmoAem83T1qO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Convert(string): \n",
        "    str1=string.split(\"\\r\\n\")\n",
        "    return str1 \n",
        "\n",
        "list_Rudyard = []\n",
        "list_Rudyard=Convert(TOC_Rudyard)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjsqyNvqOdOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create Raw_Rudyard data by removing data above the first shorty story\n",
        "first_occ_Rudyard=raw_Rudyard.find(\"THE BUTTERFLY THAT STAMPED\")\n",
        "temp=len(\"THE BUTTERFLY THAT STAMPED\")\n",
        "last_occ_Rudyard=raw_Rudyard.find(\"End of the Project Gutenberg EBook of Just So Stories, by Rudyard Kipling\")\n",
        "raw_Rudyard=raw_Rudyard[(first_occ_Rudyard+temp):last_occ_Rudyard]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etMLv2XLf-zS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l=raw_Anton.find(\"CONTENTS\")\n",
        "raw_Anton=raw_Anton[l:]\n",
        "l=raw_Anton.find(\"THE SCHOOLMISTRESS\")\n",
        "e=raw_Anton.find(\"THE SHOEMAKER AND THE DEVIL\")\n",
        "temp=len(\"THE SHOEMAKER AND THE DEVIL\")\n",
        "TOC_Anton=raw_Anton[l:(e+temp)]\n",
        "list_Anton=Convert(TOC_Anton)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvV3CYMNm9Wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create Raw_Anton data by removing data above the first shorty story\n",
        "first_occ_Anton=raw_Anton.find(\"THE SHOEMAKER AND THE DEVIL\")\n",
        "temp=len(\"THE SHOEMAKER AND THE DEVIL\")\n",
        "last_occ_Anton=raw_Anton.find(\"End of the Project Gutenberg EBook of The Schoolmistress and Other Stories\")\n",
        "raw_Anton=raw_Anton[(first_occ_Anton+temp):last_occ_Anton]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck2T8KPQdpBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Stories_data(list,raw_data):\n",
        "  story_data = []\n",
        "  list1=list\n",
        "  lastno=len(list1)\n",
        "  for i in list1:\n",
        "    first = list1.index(i)\n",
        "    second = first+1\n",
        "    if second<lastno:\n",
        "      text1=list1[first].strip()\n",
        "      text2=list1[second].strip()\n",
        "      startlength = raw_data.find(text1)\n",
        "      endlength= raw_data.find(text2)\n",
        "      story=raw_data[startlength:endlength]\n",
        "      #story_data.append(story)\n",
        "      story_data.insert(first,story)\n",
        "    else:\n",
        "      text1=list1[first].strip()\n",
        "      startlength = raw_data.find(text1)\n",
        "      story=raw_data[startlength:]\n",
        "      #story_data.append(story)\n",
        "      story_data.insert(first,story)\n",
        "  return story_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ19WVsUkY22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Rudyard_Stories_Data=Stories_data(list_Rudyard,raw_Rudyard)\n",
        "Anton_Stories_Data=Stories_data(list_Anton,raw_Anton)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H22XJU-nQoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(Rudyard_Stories_Data[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlKp8_rY5NjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(Anton_Stories_Data[20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0jNBlwMwfqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Total no of words in Stories\n",
        "def NoofWords_in_Story(Stories_Data):\n",
        "  length=[]\n",
        "  for i in Stories_Data:\n",
        "    length.insert(Stories_Data.index(i),len(i))\n",
        "  return length\n",
        "Rudyard_NoofWords = NoofWords_in_Story(Rudyard_Stories_Data)\n",
        "Anton_NoofWords = NoofWords_in_Story(Anton_Stories_Data)\n",
        "print(Rudyard_NoofWords)\n",
        "print(Anton_NoofWords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pq1l7kY8J9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(sorted(set(Rudyard_Stories_Data[2])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZSxTvkH5BWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unique words in a story\n",
        "def Uniquewords(Stories_Data):\n",
        "  uniquewords_length = []\n",
        "  for i in Stories_Data:\n",
        "      print(len(sorted(set(Stories_Data[Stories_Data.index(i)]))))\n",
        "      uniqewords_length.insert(Stories_Data.index(i),len(sorted(set(Stories_Data[Stories_Data.index(i)]))))\n",
        "  return uniqewords_length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd1KK5eM6fVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Rudyard_Uniquewords = Uniquewords(Rudyard_Stories_Data)\n",
        "Anton_Uniquewords = Uniquewords(Anton_Stories_Data)\n",
        "print(Rudyard_Uniquewords)\n",
        "print(Anton_Uniquewords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZqSFNJm6ZEP",
        "colab_type": "code",
        "outputId": "30c19883-507a-46dc-8aa3-f9a13b85e373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#More than 10 length words\n",
        "RudyardMoretenwords =[]\n",
        "AntonMoretenwords =[]\n",
        "for word in Rudyard_unique:\n",
        "  if len(word)>10:\n",
        "    RudyardMoretenwords.append(word)\n",
        "    \n",
        "for word in Anton_unique:\n",
        "  if len(word)>10:\n",
        "    AntonMoretenwords.append(word)\n",
        "    \n",
        "Rudyard_Morethan10char= len(RudyardMoretenwords)\n",
        "Anton_Morethan10char= len(AntonMoretenwords)\n",
        "Rudyard_Morethan10char,Anton_Morethan10char"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erbxeSNCDrfD",
        "colab_type": "code",
        "outputId": "9ab65521-aef4-4d3b-d9a4-855af9ee36f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# count special character\n",
        "import re\n",
        "SpecialRudyard = (re.sub('[\\w]+' ,'',raw_Rudyard))\n",
        "AvgSpecialRudyard=len(SpecialRudyard)/len(raw_Rudyard)\n",
        "SpecialAnton = (re.sub('[\\w]+' ,'',raw_Anton))\n",
        "AvgSpecialAnton=len(SpecialAnton)/len(raw_Anton)\n",
        "AvgSpecialRudyard,AvgSpecialAnton"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.22646173688736027, 0.2292340126493324)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIC7IE8lmFDA",
        "colab_type": "text"
      },
      "source": [
        "## Stage 2: Experiment with Handcrafted features representation\n",
        "\n",
        "**Stylometry: ** is the quantitative study of literary style through computational distant reading methods. It is based on the observation that authors tend to write in relatively consistent, recognizable and unique ways. For example:\n",
        "\n",
        "Each person has their own unique vocabulary, sometimes rich, sometimes limited. Although a larger vocabulary is usually associated with literary quality, this is not always the case. Ernest Hemingway is famous for using a surprisingly small number of different words in his writing,1 which did not prevent him from winning the Nobel Prize for Literature in 1954.\n",
        "Some people write in short sentences, while others prefer long blocks of text consisting of many clauses.\n",
        "No two people use semicolons, em-dashes, and other forms of punctuation in the exact same way.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**You may explore the following ways to analyze the text and generate handcrafted features by searching text in a probing way:**\n",
        "\n",
        "a) Could  style of punctuation usage help as an handcrafted feature? Both by those who follow punctuations and by those who don't? Interesting [link](https://qwiklit.com/2014/03/05/top-10-authors-who-ignored-the-basic-rules-of-punctuation/) \n",
        "\n",
        "b) The same word can sometimes be used in different contexts repeatedly by different authors. Could this fact be converted as a handcrafted feature? [link](https://www.nltk.org/book/ch01.html)\n",
        "\n",
        "c) The above two are merely examples; As you might have noticed already the NLTK book [link](https://www.nltk.org/book/) offers several methods of analyzing and understanding text. Each of these analyses is in itself capable of being an handcrafted feature. **However for your evaluation a minimal set of useful handcrafted features which is helping you prove a classification of an is sufficient**\n",
        "\n",
        "d) Could most command words be used to distinguish authors?  Refer \"Counting Vocabulary\" section of the [link](https://www.nltk.org/book/ch01.html)\n",
        "\n",
        "e) How about using a count of most frequently used bi-gram, tri-grams and using it to classify an author?\n",
        "\n",
        "f) How about using the frequency historgram of the most frequently used words across the stories by a given author a useful feature? \n",
        "\n",
        "The limit here is endless limited only by your imagination, and of course your accuracy ! :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmJX5Jl1CRKg",
        "colab_type": "text"
      },
      "source": [
        "### 2 Marks-> a) Identify 10 handcrafted features of the kind described by the  story above\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUJlJdy_EGAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A simple example: \n",
        "#If a particular book by Shakespeare, has x unique words, y average sentence lenght then the feature vector for this author would look as follows:\n",
        "\n",
        "# UniqueWords    AvgSentLenth     Label \n",
        "#      x             y              1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsmXX3tKlZIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "fdist_Rudyyard_train=FreqDist(tokens_Rudyard_train)\n",
        "#dist_Rudyyard_train.most_common(50)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VzunoGiEau5",
        "colab_type": "text"
      },
      "source": [
        "###  5 Marks-> b) Write functions for generated 8 of the above 10 features you described"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiJOpSJ0yJod",
        "colab_type": "text"
      },
      "source": [
        "## Stage 3: Experiment with text processing and representation:\n",
        "TFIDF or CountVectorizer or Word2vec features \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ecMmcZfm_Eek"
      },
      "source": [
        "### 3 Marks->  a) Performing basic cleanup operations such as removing the newline characters and removing trailing spaces\n",
        "For example: If you text looks as follows \\[' This is a sentence\\n\\r. Another sentence \\n'], after newline removal your sentence would look like \\['This is sentence a . Another sentence'] . In order to do this you can try using a combination of split() and join()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vw2DhQGK_Eel",
        "colab": {}
      },
      "source": [
        "##################### YOUR CODE HERE ##################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A5cmTI9-_Eeq"
      },
      "source": [
        "### 3 Marks-> b) Tokenize the sentence\n",
        "**Example of a tokenized sentence:** <br/>Using same example from the previous cell, after tokenization the sentence <br/> \\['This is sentence a . Another sentence']  would now be split into two sentences i.e. two elements in the list as follows:  \\['This is a sentence .' , ' Another sentence']  \n",
        "<br/>\n",
        "You may further choose to break sentences to individual words **if needed. **<br/>\n",
        "**Example of sentences as list of words:**<br/>\n",
        "**Before:** ['This is a sentence .' , ' Another sentence']<br/>\n",
        "**After:** ['This', 'is' ,'a', 'sentence' , ' . ' , ' Another ', ' sentence ' ]\n",
        "\n",
        "Hint: Consider using 'List comprehensions', whereby you split the sentence with spaces (using split('')) and populate the resulting output a list comprehension.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lgEtwoYw_Eet"
      },
      "source": [
        "###  1 Mark-> c) Is stop word removal necessary in the context of author identification? Your thoughts below?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mLrl-HCA_Eeu",
        "colab": {}
      },
      "source": [
        "###### YOUR ANSWER IN TEXT##########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z64gLpY2_Ee1"
      },
      "source": [
        "###  4 Marks-> c) Construct a labelled dataset of sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbkfSFT2xXJY",
        "colab_type": "text"
      },
      "source": [
        "To create a representation of text, convert it into vectors (numbers).** Use any one ** of the following algorithms for the task :\n",
        "\n",
        "* Countvectorizer or\n",
        "* TFIDFVectorizer or \n",
        "* Word2Vec \n",
        "\n",
        "References Documents: \n",
        "\n",
        "1.   [Countvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "2.  [TFIDFVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyk9-3BRhCf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################ YOUR CODE HERE###########################\n",
        "\n",
        "#Define a vectorizer\n",
        "\n",
        "#fit_transform your data to teh vectorizer\n",
        "\n",
        "#Convert to numpy array if needed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QhnnX13-bwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9eqZubqiRKi",
        "colab_type": "text"
      },
      "source": [
        "## 4 Marks-> Stage 4: Classification :\n",
        "Perform a classification using either features obtained from Step2 or Step3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKmfqV25a05p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################YOUR CODE HERE###############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXGRHmvSJWRd",
        "colab_type": "text"
      },
      "source": [
        "# Further Ideas for exploration after the hackathon:\n",
        "\n",
        "**Statistical analysis** of text using NLP, by analysis meaning of sentences, feature based grammars and analyzing structure of sentences! For all of this you can reference www.nltk.org/book"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g58h65mTksRp",
        "colab_type": "text"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8XIzcPTkt4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEVU8zoYkwZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \" NO\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHaOHS-jkzzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"Yes\", \"No\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMMWMbdGk2se",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id =return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwS817hvk52p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}